---
title: "F1 Tire Degradation Model"
author: "Matthew Kirk"
date: "`r Sys.Date()`"
format:
  html:
    code-tools: true

editor: visual
execute-dir: project
---

```{r setup}
#| context: setup
#| include: false
#| echo: false
# This chunk is only here to set the default chunk type to R
require("knitr")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

## Project Summary

Use race lap times to create predictive tire falloff models for each driver, to use live in future races.

This project uses data from the [FastF1](https://docs.fastf1.dev/examples/index.html) Python library, and imports it into the R environment for analysis.

This project also used the [Ergast F1 API](http://ergast.com/mrd/) for initial EDA and simplified models.

------------------------------------------------------------------------

**TODO:**

-   Normalise lap time some how? First lap = 1 or something?

    -   Get circuit lap length? Longer lap = more wear per lap?

-   *Extension:* use this tire model to create simulated pit stop prediction models/visualisations (using D3.js).

## Introduction

### Data Sources

Initially, I used the Ergast F1 API as my main source of data, as it was an easy to use API that I could access with simple R tools. However, I soon realised that Ergast was missing a whole lot of data that I was interested in for this project.

Most importantly, the Ergast API did not track the tire compounds used. This is probably one of the most important pieces of information when trying to model tire degradation, as different compounds may have different falloff properties.

I was also interested in weather, which Ergast did not track.

Finally, my long term plan is to have this model run live during a race. Since the Ergast data is only updated hours after a session, it was never going to be the 'final' data source for this project.

For all these reasons, I decided to use the FastF1 Python library. I was initially hesitant to do this, because I want to do this project in R, and the Ergast API was much faster to pick up. But crucially, the FastF1 API has so much more data, and it has functionality to live stream the data during sessions, which would be useful later in the project when it comes to running these models live. For these reasons I decided I may as well go with FastF1.

Below is the my code for accessing the data. Note, this script is designed to be run from the command line with one argument (year). The program loops through every race of the season and outputs a single parquet file.

```{python}
#| echo: true
#| eval: false
#| file: get_race_data.py
#| code-fold: true
```

## Packages

```{r}
#| label: r-load-packages
#| include: true
#| output: false


library(tidyverse)
library(lubridate)
library(jsonlite)
library(knitr)
library(arrow)
library(GGally)
```

## Import Lap Time & Weather Data

```{r}
#| label: folder-structure-settings


DATA_PATH <- "data/"
YEAR <- 2023
```

```{r}
# Get FastF1 data from exported dataframe
raw_df <- read_parquet(paste0(DATA_PATH, "races_", YEAR, ".parquet"))
head(raw_df)
```

## Missing Lap Time Data

As you can see below, there are quite a few missing laptimes in the dataset. A lot of these are due to changes in TrackStatus. This denotes when theres been a safety car, yellow flag, red flag etc. These are laps that are not particularly useful for our analysis. However there are a few missing laptimes not under special track conditions.

```{r}
missing_laptimes <- raw_df %>%
  filter(is.na(LapTime)) %>%
  arrange(Season, Round, LapNumber)

count(missing_laptimes)

head(missing_laptimes)

```

The dataset includes sector times for each lap. Therefore, we can sum these sector times to derive the missing lap times. (I checked before that the summation of sector times only varies from the recorded laptime by tiny fractions of a second).

```{r}
a_raw_df <- raw_df %>% 
  mutate(LapTime = ifelse(is.na(LapTime),
                          as.numeric(Sector1Time + Sector2Time + Sector3Time),
                          LapTime))
still_missing_laps <- a_raw_df %>%
  filter(is.na(LapTime),
         is.na(TrackStatus) | !str_detect(as.character(TrackStatus), "[4567]"))
still_missing_laps
```

This leaves 40 laps under normal (or unknown) racing conditions.

There are a significant number of missing laps in round 3 (The Australian GP). These missing laps correspond to the laps when a red flag was flown, and the race was suspended, and the following lap which is an outlap. Thus, neither of these laps are under racing conditions. Thus I will leave those laps as undefined. The other missing lap times seem to correspond to the exact lap a driver retired from the race.

```{r}
laptime_df <- a_raw_df %>%
  select(Driver, DriverNumber, LapTime, LapNumber, Position, Stint, PitOutTime, PitInTime, Compound, TyreLife, FreshTyre, Team, TrackStatus, Time, AirTemp, TrackTemp, Humidity, Pressure, Rainfall, Round, Season) %>%
  rename(CompoundSimple = Compound) %>%
  mutate(LapTime = as.numeric(LapTime))

head(laptime_df)
```

## Get Circuit Data

The following code access the Race Schedule endpoint of the Ergast F1 API. It then accesses the Wikipedia article for the corresponding race and gets track details such as lap length, and type (street course, permanent faculty). Finally, it writes locally to a parquet file.

```{r}
#| echo: true
#| eval: false
#| file: get_circuit_data.R
#| code-fold: true
```

Import circuits dataframe.Â 

```{r}
circuits_df <- read_parquet(paste0(DATA_PATH, "circuits_", YEAR, ".parquet")) %>%
  select(-url)

head(circuits_df)
```

Join circuits dataframe to laptime dataframe:

```{r}
a_laptime_df <- laptime_df %>%
  left_join(circuits_df, by = c("Season", "Round"))

head(laptime_df)
```

## Get Tire Compound Data

I manually created a csv file with the tire compounds (e.g. C0, C1, ... C5) for each round of the past 2 seasons.

```{r}
compound_df <- read_csv(paste0(DATA_PATH, "tire_compounds.csv"))
head(compound_df)
```

```{r}
compound_pivot_df <- compound_df %>%
  pivot_longer(cols = c(SOFT, MEDIUM, HARD), names_to = "CompoundSimple", values_to = "Compound_C")

b_laptime_df <- a_laptime_df %>%
  left_join(compound_pivot_df, by = c("Season", "Round", "CompoundSimple"))

c_laptime_df <- b_laptime_df %>%
  mutate(Compound = case_when(
    CompoundSimple == "INTERMEDIATE" ~ "INTER",
    CompoundSimple == "WET" ~ "WET",
    TRUE ~ Compound_C)) %>%
  select(-Compound_C)

head(laptime_df)
```

```{r}
c_laptime_df %>%
  group_by(Compound) %>%
  summarise(Laps = n())
```

## Test Plot

```{r}
temp <- c_laptime_df %>%
  group_by(Round, Driver, Stint) %>%
  mutate(StintLaps = 1) %>%
  mutate(StintLaps = cumsum(StintLaps))
```

```{r}
temp %>%
  filter(
    Driver == "LEC",
    Round == 1,
    is.na(PitOutTime),   # NOT pit in lap (slower)
    is.na(PitInTime),    # NTO pit out lap (slower)
    #SafteyCar,Yellow,RedFlag
    !str_detect(as.character(TrackStatus), "[4567]"),
    Compound != "INTER",
    Compound != "WET") %>%  
  ggplot() + 
  geom_point(aes(x = TyreLife, y = LapTime, group = Stint, colour = CompoundSimple), alpha = 0.4) + 
  geom_smooth(aes(x = TyreLife, y = LapTime, group = CompoundSimple, colour = CompoundSimple))

```

## Test Analysis

I need to normalise laptime and lap length ?

```{r}
model1 <- lm(LapTime ~ DriverNumber + LapNumber + TrackTemp + Rainfall+ CircuitId + circuitType + Compound, data = c_laptime_df)

plot(model1)

summary(model1)
```
